{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwpnoYmfJUOt"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness && pip install -e .\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN56fLvORdqA"
      },
      "outputs": [],
      "source": [
        "# Import and deploy Qwen models\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import sys\n",
        "import deepspeed\n",
        "\n",
        "sys.path.append(\"/content/lm-evaluation-harness\")\n",
        "from lm_eval import evaluator, tasks\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\"\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": 8,\n",
        "    \"train_micro_batch_size_per_gpu\": 8\n",
        "}\n",
        "\n",
        "MODELS = [\n",
        "    \"Qwen/Qwen2.5-1.5B\",\n",
        "    \"Qwen/Qwen2.5-7B\"\n",
        "]\n",
        "\n",
        "TASKS = {\n",
        "    \"NLI\": [\"hellaswag\"],\n",
        "    \"understanding\": [\"mmlu\"],\n",
        "    \"code_generation\": [\"mbpp\"]\n",
        "}\n",
        "\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV6f-GxkKBg9",
        "outputId": "acf0caa3-829f-4a24-ff3e-1cf9ab7ddf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running loglikelihood requests:  41%|████      | 16277/40168 [02:28<03:13, 123.33it/s]"
          ]
        }
      ],
      "source": [
        "# Benchmark for Qwen\n",
        "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
        "\n",
        "for model_name in MODELS:\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    ds_engine = deepspeed.init_inference(\n",
        "        model=model,\n",
        "        mp_size=1,\n",
        "        dtype=torch.float16,\n",
        "        replace_with_kernel_inject=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    hf_model = HFLM(\n",
        "        pretrained=ds_engine.module,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=8,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "\n",
        "    model_results = {}\n",
        "\n",
        "    for category, task_list in TASKS.items():\n",
        "        print(f\"Evaluating {category} tasks...\")\n",
        "\n",
        "        num_fewshot = 2 if category == \"code_generation\" else 0\n",
        "\n",
        "        results_dict = evaluator.simple_evaluate(\n",
        "            model=hf_model,\n",
        "            tasks=task_list,\n",
        "            num_fewshot=num_fewshot,\n",
        "            batch_size=8,\n",
        "            device=\"cuda\",\n",
        "            confirm_run_unsafe_code=True,\n",
        "            gen_kwargs=\"temperature=0.1,top_p=0.95,max_length=512\",\n",
        "            random_seed=42,\n",
        "            torch_random_seed=42,\n",
        "            fewshot_random_seed=42\n",
        "        )\n",
        "\n",
        "        model_results[category] = results_dict\n",
        "\n",
        "    results[model_name] = model_results\n",
        "\n",
        "    del hf_model\n",
        "    del ds_engine\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VM52HFALisUA"
      },
      "outputs": [],
      "source": [
        "# Value Function\n",
        "def get_metric_value(task_results, metric_name):\n",
        "    formats = [\n",
        "        f\"{metric_name},none\",\n",
        "        metric_name\n",
        "    ]\n",
        "\n",
        "    for fmt in formats:\n",
        "        if fmt in task_results:\n",
        "            return task_results[fmt]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLhD2BzQX5P1"
      },
      "outputs": [],
      "source": [
        "# Print Model Evaluation Results\n",
        "print(\"\\n============= Model Evaluation Results with 7B vs 1.5B Improvement =============\")\n",
        "\n",
        "# Determine all task categories\n",
        "all_categories = set()\n",
        "for model_results in results.values():\n",
        "    all_categories.update(model_results.keys())\n",
        "\n",
        "# Get models in correct order for comparison\n",
        "models = list(results.keys())\n",
        "model_headers = [model.split('/')[-1] for model in models]  # Only take the last part of model names\n",
        "\n",
        "# Ensure we have exactly 2 models for comparison\n",
        "if len(models) != 2:\n",
        "    print(\"Warning: Expected exactly 2 models for comparison\")\n",
        "\n",
        "# Find the indices for the 1.5B and 7B models\n",
        "model_1_5B_idx = -1\n",
        "model_7B_idx = -1\n",
        "for i, header in enumerate(model_headers):\n",
        "    if \"1.5B\" in header:\n",
        "        model_1_5B_idx = i\n",
        "    elif \"7B\" in header:\n",
        "        model_7B_idx = i\n",
        "\n",
        "# Print headers\n",
        "header = \"Task/Metric\".ljust(25)\n",
        "for model_header in model_headers:\n",
        "    header += model_header.ljust(20)\n",
        "header += \"Improvement(pp)\".ljust(20)  # Add improvement column\n",
        "print(header)\n",
        "print(\"-\" * (25 + 20 * len(models) + 20))  # Extend line for new column\n",
        "\n",
        "# Process and print results for all categories\n",
        "for category in sorted(all_categories):\n",
        "    print(f\"\\n【{category}】\")\n",
        "\n",
        "    # Collect all tasks in this category\n",
        "    category_tasks = set()\n",
        "    for model in models:\n",
        "        if category in results[model] and 'results' in results[model][category]:\n",
        "            category_tasks.update(results[model][category]['results'].keys())\n",
        "\n",
        "    # Print results for each task\n",
        "    for task in sorted(category_tasks):\n",
        "        # Special handling for hellaswag - distinguish between standard and normalized\n",
        "        if task == \"hellaswag\":\n",
        "            # Standard acc\n",
        "            task_line = f\"  {task} (standard)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "            # Normalized acc\n",
        "            task_line = f\"  {task} (normalized)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc_norm\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_norm_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "        # Special handling for mbpp\n",
        "        elif task == \"mbpp\":\n",
        "            task_line = f\"  {task} (pass@1)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"pass_at_1\")\n",
        "                    stderr = get_metric_value(task_results, \"pass_at_1_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "        # Handle MMLU and other tasks that use acc\n",
        "        else:\n",
        "            task_line = f\"  {task}\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}