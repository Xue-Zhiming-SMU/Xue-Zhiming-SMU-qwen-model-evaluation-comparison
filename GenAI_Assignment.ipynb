{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xue-Zhiming-SMU/Xue-Zhiming-SMU-qwen-model-evaluation-comparison/blob/main/GenAI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwpnoYmfJUOt"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness && pip install -e .\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN56fLvORdqA"
      },
      "outputs": [],
      "source": [
        "# Import and deploy Qwen models\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import sys\n",
        "import deepspeed\n",
        "\n",
        "sys.path.append(\"/content/lm-evaluation-harness\")\n",
        "from lm_eval import evaluator, tasks\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\"\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": 8,\n",
        "    \"train_micro_batch_size_per_gpu\": 8\n",
        "}\n",
        "\n",
        "MODELS = [\n",
        "    \"Qwen/Qwen2.5-1.5B\",\n",
        "    \"Qwen/Qwen2.5-7B\"\n",
        "]\n",
        "\n",
        "TASKS = {\n",
        "    \"NLI\": [\"hellaswag\"],\n",
        "    \"understanding\": [\"mmlu\"],\n",
        "    \"code_generation\": [\"mbpp\"]\n",
        "}\n",
        "\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV6f-GxkKBg9"
      },
      "outputs": [],
      "source": [
        "# Benchmark for Qwen\n",
        "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
        "\n",
        "for model_name in MODELS:\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    ds_engine = deepspeed.init_inference(\n",
        "        model=model,\n",
        "        mp_size=1,\n",
        "        dtype=torch.float16,\n",
        "        replace_with_kernel_inject=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    hf_model = HFLM(\n",
        "        pretrained=ds_engine.module,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=8,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "\n",
        "    model_results = {}\n",
        "\n",
        "    for category, task_list in TASKS.items():\n",
        "        print(f\"Evaluating {category} tasks...\")\n",
        "\n",
        "        num_fewshot = 2 if category == \"code_generation\" else 0\n",
        "\n",
        "        results_dict = evaluator.simple_evaluate(\n",
        "            model=hf_model,\n",
        "            tasks=task_list,\n",
        "            num_fewshot=num_fewshot,\n",
        "            batch_size=8,\n",
        "            device=\"cuda\",\n",
        "            confirm_run_unsafe_code=True,\n",
        "            gen_kwargs=\"temperature=0.1,top_p=0.95,max_length=512\",\n",
        "            random_seed=42,\n",
        "            torch_random_seed=42,\n",
        "            fewshot_random_seed=42\n",
        "        )\n",
        "\n",
        "        model_results[category] = results_dict\n",
        "\n",
        "    results[model_name] = model_results\n",
        "\n",
        "    del hf_model\n",
        "    del ds_engine\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VM52HFALisUA"
      },
      "outputs": [],
      "source": [
        "# Value Function\n",
        "def get_metric_value(task_results, metric_name):\n",
        "    formats = [\n",
        "        f\"{metric_name},none\",\n",
        "        metric_name\n",
        "    ]\n",
        "\n",
        "    for fmt in formats:\n",
        "        if fmt in task_results:\n",
        "            return task_results[fmt]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLhD2BzQX5P1",
        "outputId": "2c86e4ee-ddb1-4a61-fd7c-d32c53bcae85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============= Model Evaluation Results with 7B vs 1.5B Improvement =============\n",
            "Task/Metric              Qwen2.5-1.5B        Qwen2.5-7B          Improvement(pp)     \n",
            "-------------------------------------------------------------------------------------\n",
            "\n",
            "【NLI】\n",
            "  hellaswag (standard)   50.24% ± 0.50%      60.01% ± 0.49%      +9.77pp             \n",
            "  hellaswag (normalized) 67.75% ± 0.47%      78.93% ± 0.41%      +11.18pp            \n",
            "\n",
            "【code_generation】\n",
            "  mbpp (pass@1)          46.00% ± 2.23%      62.20% ± 2.17%      +16.20pp            \n",
            "\n",
            "【understanding】\n",
            "  mmlu                   59.74% ± 0.39%      71.90% ± 0.35%      +12.16pp            \n",
            "  mmlu_abstract_algebra  35.00% ± 4.79%      54.00% ± 5.01%      +19.00pp            \n",
            "  mmlu_anatomy           51.85% ± 4.32%      71.85% ± 3.89%      +20.00pp            \n",
            "  mmlu_astronomy         71.05% ± 3.69%      83.55% ± 3.02%      +12.50pp            \n",
            "  mmlu_business_ethics   61.00% ± 4.90%      76.00% ± 4.29%      +15.00pp            \n",
            "  mmlu_clinical_knowledge68.30% ± 2.86%      76.98% ± 2.59%      +8.68pp             \n",
            "  mmlu_college_biology   66.67% ± 3.94%      83.33% ± 3.12%      +16.67pp            \n",
            "  mmlu_college_chemistry 40.00% ± 4.92%      54.00% ± 5.01%      +14.00pp            \n",
            "  mmlu_college_computer_science47.00% ± 5.02%      71.00% ± 4.56%      +24.00pp            \n",
            "  mmlu_college_mathematics38.00% ± 4.88%      52.00% ± 5.02%      +14.00pp            \n",
            "  mmlu_college_medicine  63.01% ± 3.68%      68.79% ± 3.53%      +5.78pp             \n",
            "  mmlu_college_physics   39.22% ± 4.86%      50.00% ± 4.98%      +10.78pp            \n",
            "  mmlu_computer_security 73.00% ± 4.46%      83.00% ± 3.78%      +10.00pp            \n",
            "  mmlu_conceptual_physics58.30% ± 3.22%      73.19% ± 2.90%      +14.89pp            \n",
            "  mmlu_econometrics      44.74% ± 4.68%      62.28% ± 4.56%      +17.54pp            \n",
            "  mmlu_electrical_engineering62.07% ± 4.04%      69.66% ± 3.83%      +7.59pp             \n",
            "  mmlu_elementary_mathematics47.35% ± 2.57%      73.54% ± 2.27%      +26.19pp            \n",
            "  mmlu_formal_logic      46.83% ± 4.46%      55.56% ± 4.44%      +8.73pp             \n",
            "  mmlu_global_facts      29.00% ± 4.56%      42.00% ± 4.96%      +13.00pp            \n",
            "  mmlu_high_school_biology74.52% ± 2.48%      87.10% ± 1.91%      +12.58pp            \n",
            "  mmlu_high_school_chemistry47.29% ± 3.51%      65.52% ± 3.34%      +18.23pp            \n",
            "  mmlu_high_school_computer_science62.00% ± 4.88%      86.00% ± 3.49%      +24.00pp            \n",
            "  mmlu_high_school_european_history73.33% ± 3.45%      83.64% ± 2.89%      +10.30pp            \n",
            "  mmlu_high_school_geography76.26% ± 3.03%      88.89% ± 2.24%      +12.63pp            \n",
            "  mmlu_high_school_government_and_politics82.90% ± 2.72%      93.78% ± 1.74%      +10.88pp            \n",
            "  mmlu_high_school_macroeconomics68.46% ± 2.36%      78.46% ± 2.08%      +10.00pp            \n",
            "  mmlu_high_school_mathematics42.59% ± 3.01%      56.67% ± 3.02%      +14.07pp            \n",
            "  mmlu_high_school_microeconomics69.33% ± 3.00%      88.66% ± 2.06%      +19.33pp            \n",
            "  mmlu_high_school_physics37.09% ± 3.94%      60.26% ± 4.00%      +23.18pp            \n",
            "  mmlu_high_school_psychology82.02% ± 1.65%      89.36% ± 1.32%      +7.34pp             \n",
            "  mmlu_high_school_statistics47.22% ± 3.40%      68.98% ± 3.15%      +21.76pp            \n",
            "  mmlu_high_school_us_history73.04% ± 3.11%      86.76% ± 2.38%      +13.73pp            \n",
            "  mmlu_high_school_world_history76.79% ± 2.75%      87.34% ± 2.16%      +10.55pp            \n",
            "  mmlu_human_aging       64.13% ± 3.22%      77.13% ± 2.82%      +13.00pp            \n",
            "  mmlu_human_sexuality   70.99% ± 3.98%      82.44% ± 3.34%      +11.45pp            \n",
            "  mmlu_humanities        53.37% ± 0.67%      62.87% ± 0.64%      +9.50pp             \n",
            "  mmlu_international_law 79.34% ± 3.70%      84.30% ± 3.32%      +4.96pp             \n",
            "  mmlu_jurisprudence     76.85% ± 4.08%      83.33% ± 3.60%      +6.48pp             \n",
            "  mmlu_logical_fallacies 75.46% ± 3.38%      82.21% ± 3.00%      +6.75pp             \n",
            "  mmlu_machine_learning  43.75% ± 4.71%      62.50% ± 4.60%      +18.75pp            \n",
            "  mmlu_management        82.52% ± 3.76%      89.32% ± 3.06%      +6.80pp             \n",
            "  mmlu_marketing         84.19% ± 2.39%      93.16% ± 1.65%      +8.97pp             \n",
            "  mmlu_medical_genetics  69.00% ± 4.65%      86.00% ± 3.49%      +17.00pp            \n",
            "  mmlu_miscellaneous     72.80% ± 1.59%      85.95% ± 1.24%      +13.15pp            \n",
            "  mmlu_moral_disputes    68.50% ± 2.50%      80.64% ± 2.13%      +12.14pp            \n",
            "  mmlu_moral_scenarios   24.69% ± 1.44%      31.17% ± 1.55%      +6.48pp             \n",
            "  mmlu_nutrition         68.30% ± 2.66%      80.07% ± 2.29%      +11.76pp            \n",
            "  mmlu_other             65.47% ± 0.83%      76.76% ± 0.72%      +11.30pp            \n",
            "  mmlu_philosophy        66.88% ± 2.67%      78.46% ± 2.34%      +11.58pp            \n",
            "  mmlu_prehistory        68.83% ± 2.58%      83.64% ± 2.06%      +14.81pp            \n",
            "  mmlu_professional_accounting47.52% ± 2.98%      56.03% ± 2.96%      +8.51pp             \n",
            "  mmlu_professional_law  43.74% ± 1.27%      53.52% ± 1.27%      +9.78pp             \n",
            "  mmlu_professional_medicine61.76% ± 2.95%      77.94% ± 2.52%      +16.18pp            \n",
            "  mmlu_professional_psychology58.99% ± 1.99%      77.78% ± 1.68%      +18.79pp            \n",
            "  mmlu_public_relations  61.82% ± 4.65%      68.18% ± 4.46%      +6.36pp             \n",
            "  mmlu_security_studies  71.84% ± 2.88%      77.55% ± 2.67%      +5.71pp             \n",
            "  mmlu_social_sciences   70.82% ± 0.80%      82.58% ± 0.67%      +11.76pp            \n",
            "  mmlu_sociology         80.10% ± 2.82%      85.07% ± 2.52%      +4.98pp             \n",
            "  mmlu_stem              52.78% ± 0.86%      70.16% ± 0.79%      +17.38pp            \n",
            "  mmlu_us_foreign_policy 79.00% ± 4.09%      89.00% ± 3.14%      +10.00pp            \n",
            "  mmlu_virology          47.59% ± 3.89%      53.01% ± 3.89%      +5.42pp             \n",
            "  mmlu_world_religions   80.70% ± 3.03%      85.38% ± 2.71%      +4.68pp             \n"
          ]
        }
      ],
      "source": [
        "# Print Model Evaluation Results\n",
        "print(\"\\n============= Model Evaluation Results with 7B vs 1.5B Improvement =============\")\n",
        "\n",
        "# Determine all task categories\n",
        "all_categories = set()\n",
        "for model_results in results.values():\n",
        "    all_categories.update(model_results.keys())\n",
        "\n",
        "# Get models in correct order for comparison\n",
        "models = list(results.keys())\n",
        "model_headers = [model.split('/')[-1] for model in models]  # Only take the last part of model names\n",
        "\n",
        "# Ensure we have exactly 2 models for comparison\n",
        "if len(models) != 2:\n",
        "    print(\"Warning: Expected exactly 2 models for comparison\")\n",
        "\n",
        "# Find the indices for the 1.5B and 7B models\n",
        "model_1_5B_idx = -1\n",
        "model_7B_idx = -1\n",
        "for i, header in enumerate(model_headers):\n",
        "    if \"1.5B\" in header:\n",
        "        model_1_5B_idx = i\n",
        "    elif \"7B\" in header:\n",
        "        model_7B_idx = i\n",
        "\n",
        "# Print headers\n",
        "header = \"Task/Metric\".ljust(25)\n",
        "for model_header in model_headers:\n",
        "    header += model_header.ljust(20)\n",
        "header += \"Improvement(pp)\".ljust(20)  # Add improvement column\n",
        "print(header)\n",
        "print(\"-\" * (25 + 20 * len(models) + 20))  # Extend line for new column\n",
        "\n",
        "# Process and print results for all categories\n",
        "for category in sorted(all_categories):\n",
        "    print(f\"\\n【{category}】\")\n",
        "\n",
        "    # Collect all tasks in this category\n",
        "    category_tasks = set()\n",
        "    for model in models:\n",
        "        if category in results[model] and 'results' in results[model][category]:\n",
        "            category_tasks.update(results[model][category]['results'].keys())\n",
        "\n",
        "    # Print results for each task\n",
        "    for task in sorted(category_tasks):\n",
        "        # Special handling for hellaswag - distinguish between standard and normalized\n",
        "        if task == \"hellaswag\":\n",
        "            # Standard acc\n",
        "            task_line = f\"  {task} (standard)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "            # Normalized acc\n",
        "            task_line = f\"  {task} (normalized)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc_norm\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_norm_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "        # Special handling for mbpp\n",
        "        elif task == \"mbpp\":\n",
        "            task_line = f\"  {task} (pass@1)\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"pass_at_1\")\n",
        "                    stderr = get_metric_value(task_results, \"pass_at_1_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)\n",
        "\n",
        "        # Handle MMLU and other tasks that use acc\n",
        "        else:\n",
        "            task_line = f\"  {task}\".ljust(25)\n",
        "            model_values = []\n",
        "\n",
        "            for model in models:\n",
        "                if (category in results[model] and\n",
        "                    'results' in results[model][category] and\n",
        "                    task in results[model][category]['results']):\n",
        "                    task_results = results[model][category]['results'][task]\n",
        "\n",
        "                    # Get value and stderr\n",
        "                    value = get_metric_value(task_results, \"acc\")\n",
        "                    stderr = get_metric_value(task_results, \"acc_stderr\")\n",
        "\n",
        "                    if value is not None:\n",
        "                        # Store raw value for improvement calculation\n",
        "                        model_values.append(value)\n",
        "                        # Convert to percentage for display\n",
        "                        score = f\"{value*100:.2f}% ± {stderr*100:.2f}%\" if stderr else f\"{value*100:.2f}%\"\n",
        "                    else:\n",
        "                        model_values.append(None)\n",
        "                        score = \"N/A\"\n",
        "                    task_line += score.ljust(20)\n",
        "                else:\n",
        "                    model_values.append(None)\n",
        "                    task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            # Calculate improvement (in percentage points)\n",
        "            if len(model_values) >= 2 and model_values[model_1_5B_idx] is not None and model_values[model_7B_idx] is not None:\n",
        "                improvement = (model_values[model_7B_idx] - model_values[model_1_5B_idx]) * 100\n",
        "                task_line += f\"+{improvement:.2f}pp\".ljust(20) if improvement >= 0 else f\"{improvement:.2f}pp\".ljust(20)\n",
        "            else:\n",
        "                task_line += \"N/A\".ljust(20)\n",
        "\n",
        "            print(task_line)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}